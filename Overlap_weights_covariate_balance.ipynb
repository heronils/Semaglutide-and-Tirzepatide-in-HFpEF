{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25db061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlap Weights and Covariate Balance\n",
    "\n",
    "# This notebook calculates overlap weights and merges cohort and lab data for evaluating covariate balance study of semaglutide and tirzepatide in HFpEF.\n",
    "\n",
    "# Required inputs:\n",
    "## `cohort_all_followup.csv`\n",
    "## `cohort_all_followup_key.csv`\n",
    "## `labs_all_followup.csv`\n",
    "## `labs_all_followup_key.csv`\n",
    "\n",
    "# Output:\n",
    "## A merged DataFrame with overlap weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543292a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "cohort_all_followup = pd.read_csv(\"path/to/cohort_file.csv\")\n",
    "cohort_all_followup_key = pd.read_csv(\"path/to/cohort_file.csv\")\n",
    "\n",
    "labs_all_followup = pd.read_csv(\"path/to/cohort_file.csv\")\n",
    "labs_all_followup_key = pd.read_csv(\"path/to/cohort_file.csv\")\n",
    "\n",
    "# Create and apply a column renaming mapping for cohort_all_followup\n",
    "cohort_mapping = dict(zip(cohort_all_followup_key[\"Column Name\"], cohort_all_followup_key[\"Description\"]))\n",
    "cohort_mapping.pop('PID', None)  # Ensure 'PID' remains unchanged\n",
    "trimmed_cohort_mapping = {col: desc.split(\" - \")[-1] for col, desc in cohort_mapping.items()}\n",
    "cohort_all_followup.rename(columns=trimmed_cohort_mapping, inplace=True)\n",
    "\n",
    "# Create and apply column renaming mapping for lab columns explicitly\n",
    "labs_mapping = dict(zip(labs_all_followup_key[\"Column Name\"], labs_all_followup_key[\"Description\"]))\n",
    "selected_lab_cols = ['RUN1_ENTRY_COVARIATE_1', 'RUN1_ENTRY_COVARIATE_2', 'RUN1_ENTRY_COVARIATE_3', 'RUN1_ENTRY_COVARIATE_4', 'RUN1_ENTRY_COVARIATE_5', 'RUN1_ENTRY_COVARIATE_6', 'RUN1_ENTRY_COVARIATE_7', 'RUN1_ENTRY_COVARIATE_8', 'RUN1_ENTRY_COVARIATE_9',]\n",
    "\n",
    "lab_subset = labs_all_followup[['PID'] + selected_lab_cols].rename(\n",
    "    columns={col: labs_mapping[col].split(\" - \")[-1] for col in selected_lab_cols}\n",
    ")\n",
    "\n",
    "# Merge cohort with lab data based on 'PID'\n",
    "merged_data = pd.merge(cohort_all_followup, lab_subset, on='PID', how='left')\n",
    "\n",
    "# Calculate overlap weights\n",
    "merged_data['overlap_weight'] = merged_data.apply(\n",
    "    lambda row: 1 - row['Propensity Score'] if row['Exposure Group'] == 'E' else row['Propensity Score'], axis=1\n",
    ")\n",
    "\n",
    "# Inspect the final merged dataset\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences for each Exposure Group (R and E)\n",
    "exposure_group_counts = merged_data['Exposure Group'].value_counts()\n",
    "print(exposure_group_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Separate exposure groups\n",
    "exposed = merged_data[merged_data['Exposure Group'] == 'E'].copy()\n",
    "referent = merged_data[merged_data['Exposure Group'] == 'R'].copy()\n",
    "\n",
    "# Define ESS function\n",
    "def compute_ess(weights):\n",
    "    return (np.sum(weights) ** 2) / np.sum(weights ** 2)\n",
    "\n",
    "# Extract raw overlap weights\n",
    "weights_exp_raw = exposed['overlap_weight']\n",
    "weights_ref_raw = referent['overlap_weight']\n",
    "\n",
    "# Compute ESS for each group\n",
    "ess_exp = compute_ess(weights_exp_raw)\n",
    "ess_ref = compute_ess(weights_ref_raw)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Raw count exposed: {len(exposed):,}\")\n",
    "print(f\"Raw count referent: {len(referent):,}\")\n",
    "print(f\"ESS exposed: {ess_exp:,.1f}\")\n",
    "print(f\"ESS referent: {ess_ref:,.1f}\")\n",
    "print(f\"Reduction in sample size (exposed): {100 * (1 - ess_exp / len(exposed)):.1f}%\")\n",
    "print(f\"Reduction in sample size (referent): {100 * (1 - ess_ref / len(referent)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c10f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total sum of weights per group\n",
    "total_weights_per_group = merged_data.groupby('Exposure Group')['overlap_weight'].sum()\n",
    "print(total_weights_per_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010d40d",
   "metadata": {},
   "source": [
    "# Patient characteristics (unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "# Helper rounding functions\n",
    "def round_prop(val):\n",
    "    return Decimal(val * 100).quantize(Decimal('1.0'), rounding=ROUND_HALF_UP)\n",
    "\n",
    "def round_sd(val):\n",
    "    return round(val, 2)\n",
    "\n",
    "def round_int(val):\n",
    "    return int(round(val))\n",
    "\n",
    "# SMD helper\n",
    "def calc_smd(exp, ref):\n",
    "    mean_exp, mean_ref = np.mean(exp), np.mean(ref)\n",
    "    pooled_sd = np.sqrt((np.var(exp, ddof=1) + np.var(ref, ddof=1)) / 2)\n",
    "    return (mean_exp - mean_ref) / pooled_sd if pooled_sd else np.nan\n",
    "\n",
    "# Separate groups\n",
    "exposed = merged_data[merged_data['Exposure Group'] == 'E']\n",
    "referent = merged_data[merged_data['Exposure Group'] == 'R']\n",
    "n_exp = len(exposed)\n",
    "n_ref = len(referent)\n",
    "\n",
    "numeric_cols = merged_data.select_dtypes(include=['float64', 'int64']).columns.drop(['PID', 'Follow Up Time', 'Propensity Score', 'overlap_weight'])\n",
    "bool_cols = merged_data.select_dtypes(include=['bool']).columns\n",
    "\n",
    "categorical_split_vars = [\n",
    "    \"Number of hospitalizations (0, 1, 2 or more)\",\n",
    "    \"Number of heart failure hospitalizations (0, 1, 2 or more)\",\n",
    "    \"Weight Categorization Score\",\n",
    "    \"Entry Classification\",\n",
    "    \"Region / State\",\n",
    "    \"Race (Recategorized)\"\n",
    "]\n",
    "\n",
    "bins_labels = {\n",
    "    \"Number of hospitalizations (0, 1, 2 or more)\": ([-1, 0, 1, float('inf')], [\"0\", \"1\", \"2 or more\"]),\n",
    "    \"Number of heart failure hospitalizations (0, 1, 2 or more)\": ([-1, 0, 1, float('inf')], [\"0\", \"1\", \"2 or more\"]),\n",
    "    \"Weight Categorization Score\": ([-1, 0, 1, 2, 3], [\n",
    "        \"Class 1 Obesity (30.0-34.9)\",\n",
    "        \"Class 2 Obesity (35.0-39.9)\",\n",
    "        \"Class 3 Obesity (40.0 and above)\",\n",
    "        \"Unspecified Obesity\"\n",
    "    ]),\n",
    "    \"Entry Classification\": ([2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"]),\n",
    "    \"Region / State\": ([-1, 0, 1, 2, 3, 4], [\"Northest\", \"Midwest / North central\", \"South\", \"West\", \"Missing\"]),\n",
    "    \"Race (Recategorized)\": ([-1, 0, 1, 2, 3], [\"White\", \"Other\", \"Black\", \"Unknown / Missing\"])\n",
    "}\n",
    "\n",
    "unweighted_results = []\n",
    "\n",
    "# Binary categorical variables\n",
    "for col in bool_cols:\n",
    "    exp_count = exposed[col].sum()\n",
    "    ref_count = referent[col].sum()\n",
    "    exp_pct = exp_count / n_exp\n",
    "    ref_pct = ref_count / n_ref\n",
    "    smd = calc_smd(exposed[col].astype(int), referent[col].astype(int))\n",
    "\n",
    "    unweighted_results.append({\n",
    "        'Variable': col,\n",
    "        'Type': 'Categorical',\n",
    "        'Exposure': f\"{round_int(exp_count)} ({round_prop(exp_pct)}%)\",\n",
    "        'Referent': f\"{round_int(ref_count)} ({round_prop(ref_pct)}%)\",\n",
    "        'SMD': f\"{smd:.3f}\"\n",
    "    })\n",
    "\n",
    "# Continuous variables\n",
    "for col in numeric_cols:\n",
    "    if col in categorical_split_vars:\n",
    "        continue\n",
    "\n",
    "    mean_exp, std_exp = exposed[col].mean(), exposed[col].std()\n",
    "    median_exp, q1_exp, q3_exp = exposed[col].median(), exposed[col].quantile(0.25), exposed[col].quantile(0.75)\n",
    "\n",
    "    mean_ref, std_ref = referent[col].mean(), referent[col].std()\n",
    "    median_ref, q1_ref, q3_ref = referent[col].median(), referent[col].quantile(0.25), referent[col].quantile(0.75)\n",
    "\n",
    "    smd = calc_smd(exposed[col].dropna(), referent[col].dropna())\n",
    "\n",
    "    unweighted_results.append({\n",
    "        'Variable': col,\n",
    "        'Type': 'Continuous',\n",
    "        'Exposure': f\"{mean_exp:.2f} ± {round_sd(std_exp):.2f} [{round_int(median_exp)}, {round_int(q1_exp)}-{round_int(q3_exp)}]\",\n",
    "        'Referent': f\"{mean_ref:.2f} ± {round_sd(std_ref):.2f} [{round_int(median_ref)}, {round_int(q1_ref)}-{round_int(q3_ref)}]\",\n",
    "        'SMD': f\"{smd:.3f}\"\n",
    "    })\n",
    "\n",
    "# Expanded categorical variables\n",
    "for col in categorical_split_vars:\n",
    "    bins, labels = bins_labels[col]\n",
    "    exposed_cat = pd.cut(exposed[col], bins=bins, labels=labels)\n",
    "    referent_cat = pd.cut(referent[col], bins=bins, labels=labels)\n",
    "\n",
    "    for level in labels:\n",
    "        mask_exp = (exposed_cat == level)\n",
    "        mask_ref = (referent_cat == level)\n",
    "\n",
    "        exp_count = mask_exp.sum()\n",
    "        ref_count = mask_ref.sum()\n",
    "        exp_pct = exp_count / len(exposed_cat)\n",
    "        ref_pct = ref_count / len(referent_cat)\n",
    "\n",
    "        smd = calc_smd(mask_exp.astype(int), mask_ref.astype(int))\n",
    "\n",
    "        unweighted_results.append({\n",
    "            'Variable': f\"{col} = {level}\",\n",
    "            'Type': 'Categorical',\n",
    "            'Exposure': f\"{round_int(exp_count)} ({round_prop(exp_pct)}%)\",\n",
    "            'Referent': f\"{round_int(ref_count)} ({round_prop(ref_pct)}%)\",\n",
    "            'SMD': f\"{smd:.3f}\"\n",
    "        })\n",
    "\n",
    "# Label columns with group size\n",
    "exp_label = f\"Exposure (n = {n_exp})\"\n",
    "ref_label = f\"Referent (n = {n_ref})\"\n",
    "\n",
    "final_unweighted_table = (\n",
    "    pd.DataFrame(unweighted_results)\n",
    "    .sort_values(by=[\"Type\", \"Variable\"])\n",
    "    .reset_index(drop=True)\n",
    "    .rename(columns={\n",
    "        \"Exposure\": exp_label,\n",
    "        \"Referent\": ref_label\n",
    "    })\n",
    ")\n",
    "\n",
    "final_unweighted_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bdee6",
   "metadata": {},
   "source": [
    "# Patient characteristics (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "# Rounding helpers\n",
    "def round_prop(val):\n",
    "    return Decimal(val * 100).quantize(Decimal('1.0'), rounding=ROUND_HALF_UP)\n",
    "\n",
    "def round_sd(val):\n",
    "    return round(val, 2)\n",
    "\n",
    "def round_int(val):\n",
    "    return int(round(val))\n",
    "\n",
    "# Weighted helper functions\n",
    "def weighted_mean_std(data, weights):\n",
    "    mean = np.average(data, weights=weights)\n",
    "    variance = np.average((data - mean)**2, weights=weights)\n",
    "    return mean, np.sqrt(variance)\n",
    "\n",
    "def weighted_median_iqr(data, weights):\n",
    "    sorted_indices = np.argsort(data)\n",
    "    sorted_data = data[sorted_indices]\n",
    "    sorted_weights = weights[sorted_indices]\n",
    "    cum_weights = np.cumsum(sorted_weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "\n",
    "    median = np.interp(total_weight / 2, cum_weights, sorted_data)\n",
    "    q1 = np.interp(total_weight / 4, cum_weights, sorted_data)\n",
    "    q3 = np.interp(3 * total_weight / 4, cum_weights, sorted_data)\n",
    "\n",
    "    return median, q1, q3\n",
    "\n",
    "def weighted_smd(exp_data, exp_weights, ref_data, ref_weights):\n",
    "    mean_exp, _ = weighted_mean_std(exp_data, exp_weights)\n",
    "    mean_ref, _ = weighted_mean_std(ref_data, ref_weights)\n",
    "\n",
    "    pooled_var = (\n",
    "        np.average((exp_data - mean_exp) ** 2, weights=exp_weights) * np.sum(exp_weights) +\n",
    "        np.average((ref_data - mean_ref) ** 2, weights=ref_weights) * np.sum(ref_weights)\n",
    "    ) / (np.sum(exp_weights) + np.sum(ref_weights))\n",
    "\n",
    "    pooled_sd = np.sqrt(pooled_var)\n",
    "    return (mean_exp - mean_ref) / pooled_sd if pooled_sd else np.nan\n",
    "\n",
    "# Separate groups\n",
    "exposed = merged_data[merged_data['Exposure Group'] == 'E'].copy()\n",
    "referent = merged_data[merged_data['Exposure Group'] == 'R'].copy()\n",
    "\n",
    "# Use raw overlap weights (TSW) directly\n",
    "weights_exp_raw = exposed['overlap_weight'].values\n",
    "weights_ref_raw = referent['overlap_weight'].values\n",
    "\n",
    "# Total sum of weights for each group\n",
    "tsw_exp = np.sum(weights_exp_raw)\n",
    "tsw_ref = np.sum(weights_ref_raw)\n",
    "\n",
    "# Assign TSW‐based weights\n",
    "exposed['tsw_weight'] = weights_exp_raw\n",
    "referent['tsw_weight'] = weights_ref_raw\n",
    "\n",
    "# Identify columns\n",
    "numeric_cols = merged_data\\\n",
    "    .select_dtypes(include=['float64', 'int64'])\\\n",
    "    .columns.drop(['PID', 'Follow Up Time', 'Propensity Score', 'overlap_weight'])\n",
    "bool_cols = merged_data.select_dtypes(include=['bool']).columns\n",
    "\n",
    "categorical_split_vars = [\n",
    "    \"Number of hospitalizations (0, 1, 2 or more)\",\n",
    "    \"Number of heart failure hospitalizations (0, 1, 2 or more)\",\n",
    "    \"Weight Categorization Score\",\n",
    "    \"Entry Classification\",\n",
    "    \"Region / State\",\n",
    "    \"Race (Recategorized)\"\n",
    "]\n",
    "\n",
    "bins_labels = {\n",
    "    \"Number of hospitalizations (0, 1, 2 or more)\": ([-1, 0, 1, float('inf')], [\"0\", \"1\", \"2 or more\"]),\n",
    "    \"Number of heart failure hospitalizations (0, 1, 2 or more)\": ([-1, 0, 1, float('inf')], [\"0\", \"1\", \"2 or more\"]),\n",
    "    \"Weight Categorization Score\": ([-1, 0, 1, 2, 3], [\n",
    "        \"Class 1 Obesity (30.0-34.9)\",\n",
    "        \"Class 2 Obesity (35.0-39.9)\",\n",
    "        \"Class 3 Obesity (40.0 and above)\",\n",
    "        \"Unspecified Obesity\"\n",
    "    ]),\n",
    "    \"Entry Classification\": ([2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"]),\n",
    "    \"Region / State\": ([-1, 0, 1, 2, 3, 4], [\"Northest\", \"Midwest / North central\", \"South\", \"West\", \"Missing\"]),\n",
    "    \"Race (Recategorized)\": ([-1, 0, 1, 2, 3], [\"White\", \"Other\", \"Black\", \"Unknown / Missing\"])\n",
    "}\n",
    "\n",
    "weighted_results = []\n",
    "\n",
    "# Binary categorical variables\n",
    "for col in bool_cols:\n",
    "    exp_count = np.sum(exposed[col] * exposed['tsw_weight'])\n",
    "    exp_total = tsw_exp\n",
    "    ref_count = np.sum(referent[col] * referent['tsw_weight'])\n",
    "    ref_total = tsw_ref\n",
    "    smd = weighted_smd(\n",
    "        exposed[col].astype(int), exposed['tsw_weight'],\n",
    "        referent[col].astype(int), referent['tsw_weight']\n",
    "    )\n",
    "\n",
    "    weighted_results.append({\n",
    "        'Variable': col,\n",
    "        'Type': 'Categorical',\n",
    "        'Weighted Exposure': f\"{round_int(exp_count)} ({round_prop(exp_count / exp_total)}%)\",\n",
    "        'Weighted Referent': f\"{round_int(ref_count)} ({round_prop(ref_count / ref_total)}%)\",\n",
    "        'Weighted SMD': f\"{smd:.3f}\"\n",
    "    })\n",
    "\n",
    "# Continuous variables\n",
    "for col in numeric_cols:\n",
    "    if col in categorical_split_vars:\n",
    "        continue\n",
    "\n",
    "    exp_data = exposed[col].dropna().values\n",
    "    exp_w = exposed.loc[exposed[col].notna(), 'tsw_weight'].values\n",
    "    ref_data = referent[col].dropna().values\n",
    "    ref_w = referent.loc[referent[col].notna(), 'tsw_weight'].values\n",
    "\n",
    "    exp_mean, exp_std = weighted_mean_std(exp_data, exp_w)\n",
    "    ref_mean, ref_std = weighted_mean_std(ref_data, ref_w)\n",
    "\n",
    "    exp_median, exp_q1, exp_q3 = weighted_median_iqr(exp_data, exp_w)\n",
    "    ref_median, ref_q1, ref_q3 = weighted_median_iqr(ref_data, ref_w)\n",
    "\n",
    "    smd = weighted_smd(exp_data, exp_w, ref_data, ref_w)\n",
    "\n",
    "    weighted_results.append({\n",
    "        'Variable': col,\n",
    "        'Type': 'Continuous',\n",
    "        'Weighted Exposure': f\"{exp_mean:.2f} ± {round_sd(exp_std):.2f} [{round_int(exp_median)}, {round_int(exp_q1)}-{round_int(exp_q3)}]\",\n",
    "        'Weighted Referent': f\"{ref_mean:.2f} ± {round_sd(ref_std):.2f} [{round_int(ref_median)}, {round_int(ref_q1)}-{round_int(ref_q3)}]\",\n",
    "        'Weighted SMD': f\"{smd:.3f}\"\n",
    "    })\n",
    "\n",
    "# Expanded categorical variables\n",
    "for col in categorical_split_vars:\n",
    "    bins, labels = bins_labels[col]\n",
    "    exposed_cat = pd.cut(exposed[col], bins=bins, labels=labels)\n",
    "    referent_cat = pd.cut(referent[col], bins=bins, labels=labels)\n",
    "\n",
    "    for level in labels:\n",
    "        mask_exp = (exposed_cat == level)\n",
    "        mask_ref = (referent_cat == level)\n",
    "\n",
    "        exp_count = np.sum(exposed.loc[mask_exp, 'tsw_weight'])\n",
    "        ref_count = np.sum(referent.loc[mask_ref, 'tsw_weight'])\n",
    "        exp_pct = exp_count / tsw_exp\n",
    "        ref_pct = ref_count / tsw_ref\n",
    "\n",
    "        smd = weighted_smd(mask_exp.astype(int), exposed['tsw_weight'],\n",
    "                           mask_ref.astype(int), referent['tsw_weight'])\n",
    "\n",
    "        weighted_results.append({\n",
    "            'Variable': f\"{col} = {level}\",\n",
    "            'Type': 'Categorical',\n",
    "            'Weighted Exposure': f\"{round_int(exp_count)} ({round_prop(exp_pct)}%)\",\n",
    "            'Weighted Referent': f\"{round_int(ref_count)} ({round_prop(ref_pct)}%)\",\n",
    "            'Weighted SMD': f\"{smd:.3f}\"\n",
    "        })\n",
    "\n",
    "# Final table (now labeled by TSW)\n",
    "tsw_label_exp = f\"Weighted Exposure (n = {int(round(tsw_exp))})\"\n",
    "tsw_label_ref = f\"Weighted Referent (n = {int(round(tsw_ref))})\"\n",
    "\n",
    "final_weighted_table = (\n",
    "    pd.DataFrame(weighted_results)\n",
    "      .sort_values(by=[\"Type\", \"Variable\"])\n",
    "      .reset_index(drop=True)\n",
    "      .rename(columns={\n",
    "          \"Weighted Exposure\": tsw_label_exp,\n",
    "          \"Weighted Referent\": tsw_label_ref\n",
    "      })\n",
    ")\n",
    "\n",
    "final_weighted_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the actual weighted column names (with sample size info)\n",
    "weighted_exp_col = [col for col in final_weighted_table.columns if col.startswith(\"Weighted Exposure\")][0]\n",
    "weighted_ref_col = [col for col in final_weighted_table.columns if col.startswith(\"Weighted Referent\")][0]\n",
    "\n",
    "# Identify unweighted column names with sample size info\n",
    "unweighted_exp_col = [col for col in final_unweighted_table.columns if col.startswith(\"Exposure\")][0]\n",
    "unweighted_ref_col = [col for col in final_unweighted_table.columns if col.startswith(\"Referent\")][0]\n",
    "\n",
    "# Merge the two tables\n",
    "final_merged_table = final_unweighted_table.merge(\n",
    "    final_weighted_table,\n",
    "    on=['Variable', 'Type'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Reorder columns using the dynamic column names\n",
    "final_merged_table = final_merged_table[[\n",
    "    'Variable',\n",
    "    'Type',\n",
    "    unweighted_exp_col,\n",
    "    unweighted_ref_col,\n",
    "    'SMD',\n",
    "    weighted_exp_col,\n",
    "    weighted_ref_col,\n",
    "    'Weighted SMD'\n",
    "]]\n",
    "\n",
    "# Clean and clarify column headers (remove line break hacks)\n",
    "final_merged_table = final_merged_table.rename(columns={\n",
    "    weighted_exp_col: weighted_exp_col.replace(\"\\n\", \" \"),\n",
    "    weighted_ref_col: weighted_ref_col.replace(\"\\n\", \" \")\n",
    "})\n",
    "\n",
    "# Display final table\n",
    "final_merged_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d173c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your provided custom grouping\n",
    "custom_grouping = {\n",
    "    \"Demographics\": [\n",
    "        \"Age\", \"Gender (male)\", \"Race (Recategorized) = White\", \"Race (Recategorized) = Black\", \"Race (Recategorized) = Other\", \"Race (Recategorized) = Unknown / Missing\",\n",
    "        \"Region / State = Northest\", \"Region / State = West\", \"Region / State = Midwest / North central\", \"Region / State = South\", \"Region / State = Missing\"\n",
    "    ],\n",
    "    \"Lifestyle risk factors\": [\n",
    "        \"Smoking / Tobacco use\", \"Weight Categorization Score = Class 1 Obesity (30.0-34.9)\", \n",
    "        \"Weight Categorization Score = Class 2 Obesity (35.0-39.9)\", \"Weight Categorization Score = Class 3 Obesity (40.0 and above)\", \"Weight Categorization Score = Unspecified Obesity\"\n",
    "    ],\n",
    "    \"Diabetes complications\": [\n",
    "        \"Diabetic retinopathy\", \"Diabetic neuropathy\", \"Diabetic nephropathy\",\n",
    "        \"Diabetes with other opthalmic complications\", \"Diabetes with peripheral circulatory disorders\",\n",
    "        \"Diabetic foot\", \"Erectile dysfunction\", \"Hypoglycemia\", \"Hyperglycemia / DKA / HONK\", \"Skin infections\"\n",
    "    ],\n",
    "    \"Cardiovascular-related conditions\": [\n",
    "        \"Angina\", \"Atrial fibrillation\", \"Hypertension\", \"Hypotension\",\n",
    "        \"Hyperlipidemia\", \"MI\", \"Ischemic stroke\", \"TIA\",\n",
    "        \"Coronary atherosclerosis\", \"Cerebrovascular procedure\",\n",
    "        \"Cardiac conduction disorder\", \"Other cardiac dysrhythmia\", \"Cardiomyopathy\",\n",
    "        \"Valve disorders\", \"Edema\", \"Venous thromboembolism\",\n",
    "        \"Pulmonary hypertension\", \"Hyperkalemia\", \"Insertion of pacemakers / removal of cardiac lead\",\n",
    "        \"Implantable cardioverter defibrillator\", \"Previous cardiac procedure (CABG, PTCA, Stent)\",\n",
    "        \"PVD diagnosis or surgery\"\n",
    "    ],\n",
    "    \"Renal-related conditions\": [\n",
    "        \"CKD stage 1-2\", \"CKD stage 3-4\", \"Unspecified CKD\", \"Acute kidney injury\", \"Urolithiasis (kidney and urinary stone)\",\n",
    "        \"Hypertensive nephropathy\", \"Urinary tract infections\", \"Genital infections\"\n",
    "    ],\n",
    "    \"Other comorbidities\": [\n",
    "        \"COPD\", \"Asthma\", \"Obstructive sleep apnea\", \"Pneumonia\", \"Liver disease\",\n",
    "        \"Hyperthyroidism and other thyroid gland disorders\", \"Hypothyroidism\",\n",
    "        \"Fractures / Falls\", \"Osteoporosis\", \"Depression\", \"Dementia\",\n",
    "        \"Delirium or psychosis\", \"Anemia\", \"Influenza\", \"Serious bacterial infections\", \"MASH/MASLD\", \n",
    "        \"Urinary incontinence\", \"Biliary disease\", \"Pancreatitis\", \"Bowel obstruction\", \"Gastroparesis\"\n",
    "    ],\n",
    "    \"Diabetes medications\": [\n",
    "        \"Metformin\", \"Insulins\", \"Sulfonylureas\", \"DPP4i except sitagliptin\",\n",
    "        \"SGLT2i\", \"Any other glucose-lowering drugs\"\n",
    "    ],\n",
    "    \"Heart failure medications\": [\n",
    "        \"ACEi / ARB\", \"ARNI\", \"Thiazides\", \"Beta-blockers\", \"Calcium channel blockers\",\n",
    "        \"Digoxin  / Digitoxin\", \"Loop diuretics\", \"Other diuretics\",\n",
    "        \"Intravenous diuretics\", \"Nitrates\"\n",
    "    ],\n",
    "    \"Other medications\": [\n",
    "        \"Anti-arrhythmics\", \"Statins\", \"PCSK9 inhibitors and other lipid-lowering drugs\",\n",
    "        \"Antiplatelet agents\", \"Oral anticoagulants\", \"NSAIDs\", \"Oral corticosteroids\",\n",
    "        \"Osteporosis agents (incl. bisphosphonates)\", \"Opioids\", \"Anti-depressants\",\n",
    "        \"Antipsychotics\", \"Anxiolytics / hypnotics, benzos\", \"COPD / Asthma medications\",\n",
    "        \"Urinary tract infections antibiotics\", \"Laxatives\"\n",
    "    ],\n",
    "    \"Healthcare utilization markers\": [\n",
    "        \"Number of distinct medications\", \"Number of emergency department visits\", \"Number of office visits\",\n",
    "        \"Number of endocrinologist visits \", \"Number of cardiologist visits\",\n",
    "        \"Number of internal/family medicine visits\", \"Electrocardiograms\", \"Echocardiograms\",\n",
    "        \"Out-of-pocket medication cost\", \"Unique brand medications\", \"Unique generic medications\",\n",
    "        \"Ratio of brand to generic medications\"\n",
    "    ],\n",
    "    \"Healthy behavior markers\": [\n",
    "        \" Colonoscopy, Sigmoidoscopy\", \"Flu vaccine / Pneumococcal vaccine\", \"Pap smear\",\n",
    "        \"PSA test\", \"Fecal occult blood test\", \"Bone mineral density tests\", \"Mammograms\"\n",
    "    ],\n",
    "    \"Laboratory and diagnostic tests\": [\n",
    "        \"HbA1c tests\", \"Lipid panel\", \"Creatinine tests\", \"Urine tests\"\n",
    "    ],\n",
    "    \"Lab values\": [\n",
    "        \"BMI (Truncated)\", \"BNP (Truncated)\", \"proBNP (Truncated)\", \"HbA1c (Truncated)\",\n",
    "        \"Glucose (Truncated)\", \"eGFR (Truncated)\", \"Creatinine (Truncated)\",\n",
    "        \"Systolic blood pressure (Truncated)\", \"Heart rate (Truncated)\"\n",
    "    ],\n",
    "    \"Burden of comorbidities\": [\n",
    "        \"Combined comorbidity score (CCI), 365 days\", \"Frailty Score: Empirical Version 365 days (ICD-9/10)\"\n",
    "    ],\n",
    "    \"Baseline hospitalizations and hospital metrics\": [\n",
    "        \"Number of any hospitalization\", \"Number of heart failure hospitalizations (0, 1, 2 or more) = 0\", \"Number of heart failure hospitalizations (0, 1, 2 or more) = 1\", \"Number of heart failure hospitalizations (0, 1, 2 or more) = 2 or more\",\n",
    "        \"Number of hospitalizations (0, 1, 2 or more) = 0\", \"Number of hospitalizations (0, 1, 2 or more) = 1\", \"Number of hospitalizations (0, 1, 2 or more) = 2 or more\"\n",
    "    ],\n",
    "    \"Calendar year of cohort entry\": [\n",
    "        \"Entry Classification = 2018\", \"Entry Classification = 2019\", \"Entry Classification = 2020\", \"Entry Classification = 2021\", \"Entry Classification = 2022\", \"Entry Classification = 2023\", \"Entry Classification = 2024\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Build final ordered table\n",
    "ordered_rows = []\n",
    "for group, variables in custom_grouping.items():\n",
    "    ordered_rows.append(pd.Series({\"Variable\": group}))  # Heading\n",
    "    group_rows = final_merged_table[final_merged_table['Variable'].isin(variables)]\n",
    "    ordered_rows.extend([row for _, row in group_rows.iterrows()])\n",
    "\n",
    "# Create the final table\n",
    "final_ordered_table = pd.DataFrame(ordered_rows).reset_index(drop=True)\n",
    "\n",
    "# Display\n",
    "final_ordered_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678df6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final dataset\n",
    "merged_data.to_csv(\"merged_overlap_weights.csv\", index=False)\n",
    "\n",
    "# Optional: Plot distribution of weights\n",
    "import matplotlib.pyplot as plt\n",
    "merged_data['overlap_weight'].hist(bins=50)\n",
    "plt.title(\"Distribution of Overlap Weights\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
